from src import config
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import time
import pandas as pd
from tqdm import tqdm
from threading import Lock
from openai import OpenAI, BadRequestError, RateLimitError, APITimeoutError, APIConnectionError

def clean_json_string(text):
    """æ¸…æ´— JSON å­—ç¬¦ä¸²"""
    start = text.find('{')
    end = text.rfind('}')
    if start != -1 and end != -1:
        text = text[start : end + 1]
    return text

def call_llm_classify(title, content, retries=5):
    """
    ä½¿ç”¨ OpenAI SDK å…¼å®¹æ¨¡å¼è°ƒç”¨ Zenmux/Gemini è¿›è¡Œæ€»ç»“
    """
    client = OpenAI(
        api_key=config.API_KEY,
        base_url=config.API_URL  # ç›´æ¥ä½¿ç”¨å®Œæ•´ URLï¼Œæ— éœ€æ‰‹åŠ¨ strip "/v1"
    )
    
    user_content = f"Headline: {title}\n\nArticle Content: {content}"
    
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model=config.MODEL_NAME, # ç¡®ä¿ config ä¸­å·²æ›´æ–°ä¸º "google/gemini-3-pro-preview"
                messages=[
                    {"role": "system", "content": config.SYSTEM_PROMPT_01},
                    {"role": "user", "content": user_content}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}, # Gemini æ”¯æŒ JSON æ¨¡å¼
                timeout=120,
                stream=False
            )
            
            # 2. å¢åŠ å¯¹ finish_reason çš„æ£€æŸ¥ (Gemini æ•æ„Ÿå†…å®¹è¿‡æ»¤æœºåˆ¶)
            finish_reason = response.choices[0].finish_reason
            if finish_reason == "content_filter":
                print(f"âš ï¸ å†…å®¹å®‰å…¨æ‹¦æˆª (Gemini): {title[:15]}...")
                return None
            
            result_text = response.choices[0].message.content
            
            # ç¡®ä¿ helper å‡½æ•°å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨éœ€è¡¥å……å®šä¹‰
            return json.loads(clean_json_string(result_text))

        # 3. é”™è¯¯å¤„ç† (é€‚é…é€šç”¨ OpenAI åè®®)
        except BadRequestError as e:
            err_msg = str(e)
            # DeepSeek çš„ "Content Exists Risk" åœ¨è¿™é‡Œå¯èƒ½è¡¨ç°ä¸ºå…¶ä»– 400 é”™è¯¯
            # å¦‚æœæ˜¯å†…å®¹å®‰å…¨é—®é¢˜ï¼Œé€šå¸¸ä¸éœ€è¦é‡è¯•
            if 'safety' in err_msg.lower() or 'filter' in err_msg.lower() or 'content' in err_msg.lower():
                print(f"âš ï¸ å†…å®¹æ•æ„Ÿ/è¯·æ±‚æ‹’ç» (è·³è¿‡): {title[:15]}... é”™è¯¯ä¿¡æ¯: {e}")
                return None 
            else:
                print(f"âŒ å‚æ•°é”™è¯¯ (BadRequest): {e}")
                return None
        
        except RateLimitError:
            sleep_time = 5 * (attempt + 1)
            print(f"âš ï¸ 429 é™æµ, ç­‰å¾… {sleep_time}s...")
            time.sleep(sleep_time)
            
        except (APITimeoutError, APIConnectionError) as e:
            print(f"âš ï¸ ç½‘ç»œ/è¶…æ—¶é—®é¢˜: {e}, é‡è¯•ä¸­...")
            time.sleep(2)
            
        except json.JSONDecodeError:
            print(f"âŒ JSON è§£æå¤±è´¥ï¼Œå¯èƒ½æ˜¯æ¨¡å‹è¾“å‡ºæ ¼å¼é”™è¯¯ã€‚é‡è¯•ä¸­...")
            # å¯ä»¥é€‰æ‹©é‡è¯•ï¼Œæˆ–è€…ç›´æ¥ continue
            
        except Exception as e:
            print(f"âŒ æœªçŸ¥å¼‚å¸¸: {e}")
            time.sleep(2)
            
    return None

def llm_classify_concurrently(
    df, 
    output_csv_path=config.PROCESSED_DATA_DIR / 'classify_data.csv', 
    max_workers=None,  # é»˜è®¤ None,è®©ç³»ç»Ÿè‡ªåŠ¨å†³å®š
    save_interval=15   # æ¯å¤„ç† 15 æ¡ä¿å­˜ä¸€æ¬¡
):
    """
    å¹¶å‘å¤„ç† DataFrame,å¸¦æ€§èƒ½ç›‘æ§å’Œè¿›åº¦ä¿å­˜
    """
    
    # å®šä¹‰æ ‡å‡†çš„ 12 ä¸ªåˆæ³•åˆ†ç±»åˆ—è¡¨
    VALID_CATEGORIES = [
        "ä¸­å°è¾¹ç•Œ/è¾¹å¢ƒé—®é¢˜",
        "è¥¿è—/è¾¾èµ–å–‡å˜›é—®é¢˜",
        "å°æ¹¾é—®é¢˜",
        "ä¸€å¸¦ä¸€è·¯ä¸å‘¨è¾¹åœ°ç¼˜",
        "ä¸­å°ç»è´¸ä¸ç§‘æŠ€",
        "ä¸­å›½ç»æµç°çŠ¶",
        "ä¸­å°å†›åŠ›ä¸å›½é˜²",
        "ä¸­å›½å›½å†…æ”¿æ²»",
        "ä¸­å°åŒè¾¹å…³ç³»",
        "ä¸­å›½å¤–äº¤",
        "ä¸­å°ç­¾è¯ä¸äººæ–‡",
        "å…¶ä»–"
    ]

    # 1. åˆå§‹åŒ–åˆ—
    if 'category' not in df.columns:
        df['category'] = None
    if 'reason' not in df.columns:
        df['reason'] = None
        
    # 2. ç­›é€‰éœ€è¦å¤„ç†çš„è¡Œ
    mask_to_process = ~df['category'].isin(VALID_CATEGORIES)
    indices_to_process = df[mask_to_process].index.tolist()
    
    print(f"ğŸ“Š æ€»è¡Œæ•°: {len(df)}")
    print(f"ğŸ”„ æœ¬æ¬¡éœ€å¤„ç†: {len(indices_to_process)} è¡Œ")
    print(f"âœ… å·²å®Œæˆ: {len(df) - len(indices_to_process)} è¡Œ")
    
    if not indices_to_process:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®å·²å®Œç¾å¤„ç†å®Œæ¯•!")
        return df

    # 3. è‡ªåŠ¨è®¾ç½®çº¿ç¨‹æ•°
    if max_workers is None:
        max_workers = min(10, len(indices_to_process))  # æœ€å¤š 32 çº¿ç¨‹
    
    print(f"\nâš™ï¸ å¹¶å‘é…ç½®:")
    print(f"  - çº¿ç¨‹æ•°: {max_workers}")
    print(f"  - æ¯æ¡é‡è¯•: 5 æ¬¡")
    print(f"  - è‡ªåŠ¨ä¿å­˜é—´éš”: æ¯ {save_interval} æ¡")
    
    # 4. æ€§èƒ½ç›‘æ§
    start_time = time.time()
    completed_count = 0
    lock = Lock()  # ç”¨äºçº¿ç¨‹å®‰å…¨åœ°æ›´æ–°è®¡æ•°å™¨
    
    def update_and_save(idx, result):
        """çº¿ç¨‹å®‰å…¨çš„æ›´æ–°å’Œä¿å­˜å‡½æ•°"""
        nonlocal completed_count
        
        # ã€ä¿®æ”¹ç‚¹ 1ã€‘æ‰©å¤§ Lock èŒƒå›´ï¼ŒåŒ…å«å†™å…¥æ“ä½œï¼Œé˜²æ­¢è¯»å†™å†²çª
        with lock:
            if result: 
                df.at[idx, 'category'] = result.get('category')
                df.at[idx, 'reason'] = result.get('reason')
            else:
                df.at[idx, 'category'] = "Error"
                df.at[idx, 'reason'] = "Failed after 5 retries"
        
            completed_count += 1
            
            # å®šæœŸä¿å­˜
            if completed_count % save_interval == 0:
                # ã€ä¿®æ”¹ç‚¹ 2ã€‘å¢åŠ  try-exceptï¼Œé˜²æ­¢æ–‡ä»¶å ç”¨å¯¼è‡´ç¨‹åºå´©æºƒ
                try:
                    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
                    elapsed = time.time() - start_time
                    rate = completed_count / elapsed
                    print(f"\nğŸ’¾ å·²ä¿å­˜è¿›åº¦: {completed_count}/{len(indices_to_process)} ({rate:.2f} æ¡/ç§’)")
                except Exception as e:
                    print(f"\nâš ï¸ è‡ªåŠ¨ä¿å­˜å¤±è´¥ (ä¸å½±å“è¿è¡Œï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«å ç”¨): {e}")
    
    # 5. å¹¶å‘æ‰§è¡Œ
    print(f"\nğŸš€ å¼€å§‹å¹¶å‘å¤„ç†...\n")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_index = {
            executor.submit(
                call_llm_classify,           
                df.at[idx, 'title'],         
                df.at[idx, 'content'],
                5
            ): idx 
            for idx in indices_to_process
        }
        
        for future in tqdm(as_completed(future_to_index), total=len(indices_to_process), desc="ğŸ”¥ LLM Processing"):
            idx = future_to_index[future]
            
            try:
                result = future.result()
                update_and_save(idx, result)
                        
            except Exception as e:
                print(f"\nâŒ Row {idx} å¼‚å¸¸: {e}")
                # ã€ä¿®æ”¹ç‚¹ 3ã€‘ç¡®ä¿å¼‚å¸¸å¤„ç†æ—¶ä¹Ÿå®‰å…¨å†™å…¥å¹¶å°è¯•ä¿å­˜
                with lock:
                    df.at[idx, 'category'] = "Error"
                    df.at[idx, 'reason'] = str(e)
                    completed_count += 1
                    # ä¹Ÿå¯ä»¥åœ¨è¿™é‡ŒåŠ ä¸Šä¿å­˜é€»è¾‘ï¼Œæˆ–è€…ä¾èµ–ä¸‹ä¸€æ¬¡æˆåŠŸæ—¶çš„ä¿å­˜

    # 6. æœ€ç»ˆä¿å­˜
    try:
        df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
        print(f"\nâœ… æœ€ç»ˆä¿å­˜æˆåŠŸ!")
    except Exception as e:
        print(f"\nâŒ æœ€ç»ˆä¿å­˜å¤±è´¥: {e}")
    
    # 7. æ€§èƒ½æŠ¥å‘Š
    total_time = time.time() - start_time
    avg_rate = len(indices_to_process) / total_time
    
    print(f"\n{'='*60}")
    print(f"âœ… å¤„ç†å®Œæˆ!")
    print(f"ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
    print(f"  - æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"  - å¹³å‡é€Ÿåº¦: {avg_rate:.2f} æ¡/ç§’")
    print(f"  - å¤„ç†æ€»æ•°: {len(indices_to_process)} æ¡")
    
    # 8. æœ€ç»ˆç»Ÿè®¡
    remaining_invalid = df[~df['category'].isin(VALID_CATEGORIES)]
    if len(remaining_invalid) > 0:
        print(f"\nâš ï¸ ä»æœ‰ {len(remaining_invalid)} æ¡æœªå½’å…¥åˆæ³•åˆ†ç±»")
        print(f"   å»ºè®®: é‡æ–°è¿è¡Œæ­¤å‡½æ•°")
    else:
        print(f"\nğŸŠ å®Œç¾! æ‰€æœ‰æ•°æ®å‡å·²å½’å…¥æ ‡å‡†åˆ†ç±»")
    print(f"{'='*60}\n")
    
    return df


# ============ æµ‹è¯•å¹¶å‘æ˜¯å¦ç”Ÿæ•ˆçš„è¯Šæ–­å‡½æ•° ============

def test_concurrency(num_requests=20, max_workers=10):
    """
    å¿«é€Ÿæµ‹è¯•å¹¶å‘æ˜¯å¦çœŸæ­£ç”Ÿæ•ˆ
    è¿”å›: (æ€»è€—æ—¶, ç†è®ºå•çº¿ç¨‹è€—æ—¶, åŠ é€Ÿæ¯”)
    """
    print(f"ğŸ§ª å¹¶å‘æµ‹è¯•: {num_requests} ä¸ªè¯·æ±‚, {max_workers} ä¸ªçº¿ç¨‹\n")
    
    # æ¨¡æ‹Ÿ API è°ƒç”¨(æ¯æ¬¡è€—æ—¶ 2 ç§’)
    def mock_api_call(idx):
        time.sleep(2)  # æ¨¡æ‹Ÿ API å»¶è¿Ÿ
        return f"Result {idx}"
    
    # å•çº¿ç¨‹æµ‹è¯•
    print("1ï¸âƒ£ å•çº¿ç¨‹æµ‹è¯•...")
    start = time.time()
    for i in range(num_requests):
        mock_api_call(i)
    single_thread_time = time.time() - start
    print(f"   è€—æ—¶: {single_thread_time:.2f} ç§’\n")
    
    # å¤šçº¿ç¨‹æµ‹è¯•
    print(f"2ï¸âƒ£ {max_workers} çº¿ç¨‹æµ‹è¯•...")
    start = time.time()
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        list(executor.map(mock_api_call, range(num_requests)))
    multi_thread_time = time.time() - start
    print(f"   è€—æ—¶: {multi_thread_time:.2f} ç§’\n")
    
    # ç»“æœ
    speedup = single_thread_time / multi_thread_time
    print(f"{'='*50}")
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  å•çº¿ç¨‹è€—æ—¶: {single_thread_time:.2f} ç§’")
    print(f"  å¤šçº¿ç¨‹è€—æ—¶: {multi_thread_time:.2f} ç§’")
    print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    if speedup > 2:
        print(f"  âœ… å¹¶å‘å·¥ä½œæ­£å¸¸!")
    else:
        print(f"  âš ï¸ å¹¶å‘æ•ˆæœä¸æ˜æ˜¾,å¯èƒ½å—åˆ° API é™æµå½±å“")
    print(f"{'='*50}")
    
    return multi_thread_time, single_thread_time, speedup